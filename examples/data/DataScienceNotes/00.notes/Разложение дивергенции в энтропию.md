---
tags:
date: 2025-10-26
time: 13:25:38
location:
layer: silver
---
При помощи разложения логарифма частного можно рассмотреть [[Kullback-Leibler Divergence|дивергенцию]] как разность [[Статистическая Энтропия#Кросс-энтропия распределений|кросс энтропии]]  $H(p,q)$ и [[Статистическая Энтропия#Энтропия случайной величины|энтропии]] $H(p)$, то есть KLD описывает меру отличия информации (дополнительного [[Информация Шеннона|surprisal]]?), полученного при аппроксимации p моделью q.
$$D_{KL}(p||q) = \sum_{x\in X}\bigg[p(x)\log\frac{p(x)}{q(x)}\bigg] = H(p,q) - H(p) \leftrightarrow H(p,q) = H(p) + D_{KL}(p||q)$$
Случай $H(p) = H(p,q)$ описывает нулевой $D_{KL}$.