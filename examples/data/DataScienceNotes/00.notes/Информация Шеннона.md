---
tags:
  - STATS
layer: bronze
---
$$\boxed{I(x) = -\log(p(x)) = \log\frac{1}{p(x)}}$$
Информация Шеннона (reciprical log) измеряет "удивление" (surprise): Количество информации, связанное с событием с вероятностью p.

Логарифм обратной вероятности обратно пропорционален, т.е. для высоких вероятностей значение низко (common event) а для редких высоко (rare/very rare event):
$$
 I(x) = \log\frac{1}{p(x)}: \
 \begin{cases}
 \lim_{p(x)\rightarrow 0}\bigg\{\log\frac{1}{ p(x)} \bigg\}=+\infty \text{ - очень редкое событие несет много информации}\\
 \log\frac{1}{p(x)} = [p(x) = 1] = 0 \text{ - достоверно гарантрованно произошедшее событие не несет информации }
 \end{cases}
$$

Помимо этого информация аддитивна для независимых событий: $$p(A,B) = P(A)*P(B) \leftrightarrow \log\frac{1}{p(A,B)} = \log\frac{1}{p(A)}*\log\frac{1}{p(B)} \leftrightarrow I(A \cap B) = I(A) + I(B)$$
Это удобно описывает и [[Kullback-Leibler Divergence|разность информации от разных распределений]] через $\log\frac{ p(x)}{q(x)}$
Дополнительно логарифм по базе *n ceil* равен числу *n*ит (прим. $log_2$ и биты), достаточному для эффективного кодирования события, [[Статистическая Энтропия|энтропия]] в таком случае дополнительно описывает среднюю достаточную длину кодов для событий распределения.  