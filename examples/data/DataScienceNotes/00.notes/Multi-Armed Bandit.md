---
tags:
  - RL/Bandits
aliases:
layer: silver
---
Задача выбора оптимальной стратегии (в терминах [[RL парадигма#Policy|политики]] $\pi$) выбора действий за ограниченное конечное (?) количество шагов/попыток, при условии неизвестных заранее [[RL парадигма#Reward & Return|наград (распределений наград)]]; то есть оценка награды невозможна без принятия решения о выборе действия.

Концепт **Exploration/Exploitation** a.k.a. **Control** наиболее явен, Explore - выбрать не самое прибыльное действие (основываясь на текущей информации), чтобы получить или уточнить оценку, Exploit - *жадно* выбрать самое выгодное по имеющейся на текущий момент времени информации.

![[MAB_Doodle.png]]
В базовой формулировке MAB игнорируют $s\in\mathcal{S}$  -  *nonassociative search* в *NonContextual MAB*, то есть они имеют опущенный аргумент - $s\text{ : }\pi(a|s) = \pi(a), r(a,s) = r(a)$; [[Contextual MAB]] рассматривают полные версии $\pi(a|s), r(a,s)$.