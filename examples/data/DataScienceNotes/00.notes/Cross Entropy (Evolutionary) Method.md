---
tags:
  - RL
layer: silver
---

$$\boxed{\theta_{i+1} \sim \mathcal{N}(\overline{\theta}_i^\star, \sigma(\theta_i^\star))}$$
![[CE_evolutionary_base.png]]
Алгоритм решения задач эпизодическое РЛ ([[MDP]]) основанный на процессе отбора лучших (элитных) игр.
___
Изначально цель MDP есть $\mathbb{E}[R|\pi_{\theta}] \rightarrow max$,  задача может быть рассмотрена в следующем ключе Монте Карло оптимизации:

Есть некоторая оптимальная стохастическая стратегия $\pi_{opt} \equiv p$ поведения, ее невозможно или слишком сложно просчитать проходом по всем  возможным состояниям. Алгоритм аппроксимирует ее распределение $\pi\equiv q$, чтобы по нему максимизировать значение reward-статистики, что равнозначно минимизации $D_{KL}$
в совокупности со статистикой (IS*):
$$D_{KL}(p||q) = \mathbb{E}_p\log p - \mathbb{E}_p\log q \rightarrow min$$
$$\Downarrow$$
$$q(x) = argmin_{q(x)}\big\{ Const - \mathbb{E}_{p(x)} G(x) \log q(x)\big\}$$

*Здесь и возникает оптимизация кросс-энтропии $\uparrow$*
___
Итерация алгоритма заключается в генерации сэмплов параметров $\theta$ из распределения параметров лучших моделей в терминах [[RL парадигма#Reward-Model|reward]] прошлого шага.

Оптимизационная задача каждого шага:

$$q_{i+1}(x) = argmin_{q_{i+1}(x)}\bigg\{ -\mathbb{E}_{q_i(x)}\frac{p(x)}{q_i(x)}G(x)\log q_{i+1}(x) \bigg\}$$

$\pi(a|s) = q(x)$  - оптимизируется функция политики модели
$p(x) := Uniform(x)$ - истинное распределение полагается равномерным
$G(x) = f(R_i) = I[R_i \geq \psi_i]$ - статистика для [[Importance Sampling]] берется функцией от награды - индикатором преодоления квантильного порога пси (например, 80 перцентиль)
$w = p/q:=1$ - importance weights все положены равными единице

Итого итерация алгоритма есть: 
$$
\pi_{i+1}(a|s) = argmin_{\pi_{i+1}(a|s)}\bigg\{ -\mathbb{E}_{z\sim \pi_i(x)}[R(z) \geq \psi_i]\log \pi_{i+1}(a|s) \bigg\}
$$
Оптимизация ожидания статистики $G(x)$ приводит к росту значений $q(x)$ там, где больше эта статистика, в случае с определение индикатором здесь, метод выучивает политики, наиболее похожие на успешные в прошлом наборе эпизодов, по аналогии при обучении с разметкой оптимизация [[CrossEntropyLoss|{BSE/CE}Loss]] $G(x)$ полагается равным 0-1 вектору или ohe-меткам.

$$\theta_{i+1} \sim \mathcal{N}(\overline{\theta}_i^\star, \sigma(\theta_i^\star)) \text{,  где } \theta_i^\star = \theta_i\in\Theta_i: R(z\sim\pi_{\theta_i^\star}) \geq \psi_i$$