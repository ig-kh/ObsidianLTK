---
tags:
  - STATS
layer: bronze
---
### Энтропия случайной величины
$$\boxed{H(p) = \mathbb{E}_{x\sim p(x)}I(x) = \mathbb{E}_{x\sim p(x)}-\log(p(x))}$$
Энтропия случайной величины, равная матожидания [[Информация Шеннона|информации]] по распределению самой случайной величины, является оценкой среднего количества информации, достаточного для описания возможных значений этой случайной величины, оценивает неопределенность.
### Кросс-энтропия распределений 
$$
H(p, q) = \mathbb{E}_{x \sim p(x)} -\log (q(x))
$$
### Условная энтропия
$$H(y|x) = \sum_{x,y \in X\times Y} p_{X,Y}(x,y)\log\frac{p_{X,Y}(x,y)}{p_X(x)}$$
Понимается как остаточная случайность переменной y в зависимости от x.