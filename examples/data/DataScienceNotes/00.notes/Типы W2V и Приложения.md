---
tags:
  - NLP
layer: silver
---
Механизмы обучения [[W2V]] схожи и формируют эмбеддинги с почти похожими свойствами, однако есть разграничение:

### [[W2V#Skip-Gram|SG]]
Скип-грам эмбеддинги носят больше функциональной информации (схоже с [[Эффект Размера Контекста в W2V|малыми окнами контекста]]) о слове, более подходят для задач, связанных с ворд-левел информацией (напр. генерация) и логически вырастает в NextTokenPrediction в [[GPT]], так как связь в обучении слово-слово

### [[W2V#CBOW|CBOW]]
Эмбеддинги мешка несут больше контекстуальной информации из-за связи агрегат контекста-слово в обучении, больше подходят для смысловых задач по всему сообщению - классификации, кластеризации текстов; логика ближе к MaskedLanguageModeling в [[BERT]]




