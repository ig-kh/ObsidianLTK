---
tags:
  - RL/Bandits
layer: bronze
---
Продолжая идею [[Greedy & ɛ-Greedy Action Value-based MAB#ɛ-Greedy|стохастического выбора действия]] над текущими оценками можно построить распределение Больцмана a.k.a. Softmax и сэмплировать из него:
$$\boxed{A_t\sim\frac{\exp\{Q_t(a_i)\}/\tau}{\sum_{a\in\mathcal{A}}\exp\{Q_t(a)/\tau\}}}$$
Здесь параметр температуры $\tau\in\mathbb{R}_+$ отвечает за долю exploration, $\tau \rightarrow 0$ соответствует стремлению к жадной стратегии, так как в лимите нуля softmax редуцируется до аргмакса, $\tau\rightarrow\infty$ соответственно равен обычному равномерному, где то между отыскивается баланс.

Процедура сэмплирования действия может быть рассмотрена как [[SuttonBartoIPRLBook2ndEd.pdf#page=57|аппроксимация градиентного подъема]] 
