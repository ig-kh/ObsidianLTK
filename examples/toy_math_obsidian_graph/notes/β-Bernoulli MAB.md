---
tags:
  - RL/Bandits
  - STATS/BayessianInference
  - SYS/DEPRECATED
---
$$\boxed{\hat{\theta}_{k,t} \sim \mathcal{B}_{\alpha_{k,t},\beta_{k,t}}(\theta)}$$
Применение [[Greedy vs. Thompson Bandit|сэмплинга Томпсона]] к [[Bernoulli MAB|бандиту Бернулли]] заменяет жадную оценку параметра распределения ручки на сэмплирование из распределения специального вида.

Так как в жадной версии: $\hat{\theta} \leftarrow \frac{\alpha}{\alpha+\beta}$ является на самом деле матожиданием какого-то распределения $\hat{\theta} \leftarrow \mathbb{E}_g[\theta]$; при этом должно выполняться $\mathbb{E}_g[\theta]=\frac{\alpha}{\alpha+\beta}$.

Это выполняется для [[β-Распределение|beta-распределения]] параметризованного $\alpha$ и $\beta$, из него и производится сэмплирование: 
$$\hat{\theta}_{k,t} \sim \mathcal{B}_{\alpha_{k,t},\beta_{k,t}}(\theta)$$
В ходе обучения алгоритма с ростом $\alpha_k$ и $\beta_k$ бета распределения параметров будут формировать "колокола", повышать эксцесс, сопоставляемый с не-изученностью наград действия и сдвигать оценку параметра к истинному среднему.
![[Pasted image 20250907001216.png]]