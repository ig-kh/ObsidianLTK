---
tags:
  - NLP
  - SYS/DEPRECATED
---
В [[Дистрибутивная гипотеза языка|дистрибутивной гипотезе]] утверждается, что возможно сопоставление между самим словом и частыми его контекстами в рамках определения его семантики. 
Это позволяет строить базовые **count-based** модели (**PMI** & **TF_IDF**, например).

$$\text{e.g. } V = \begin{bmatrix}v_{i,j}\end{bmatrix} \in \mathbb{R}^{\#W\times\#C}: \begin{cases}
dox: v_{i, j} = tf-idf(d_j,w_i) = [\#w\in d_j:w = w_i] * \log\bigg[\frac{\#D}{\#D:w_i\in D }\bigg]\\
w2w: v_{i, j} = PPMI(w_i,w_j) = max\big(0, \log\frac{\mathbb{p}(w_i, w_j)}{\mathbb{p}(w_i)*\mathbb{p}(w_j)}\big)
\end{cases}$$
Для извлечения непосредственно векторов достаточно взять итоговые значения для *w*, однако более эффективно использовать [[SVD]] и в качестве эмбеддингов брать значения левой матрицы по top_k eigvals, k сопоставимо с выбранной размерностью векторизации. 
Очевидно, что слова с похожими контекстами в итоге будут иметь близкие вектора