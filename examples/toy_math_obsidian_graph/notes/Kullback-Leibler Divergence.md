---
tags:
  - STATS
  - SYS/DEPRECATED
---
### Определение
Статистическое квази-расстояние (антисимметрическая метрика) являющаяся мерой того, насколько хорошей аппроксимацией истинного распределения p является модельное q.
$$D_{KL}(p||q) = \sum_{x\in X}\bigg[p(x)\log\frac{p(x)}{q(x)}\bigg] = -\sum_{x\in X}\bigg[p(x)\log\frac{q(x)}{p(x)}\bigg]$$
### Разложение в энтропии
При помощи разложения логарифма частного можно рассмотреть дивергенцию как разность [[Статистическая Энтропия#Кросс-энтропия распределений|кросс энтропии]]  $H(p,q)$ и [[Статистическая Энтропия#Энтропия случайной величины|энтропии]] $H(p)$, то есть KLD описывает меру отличия информации (дополнительного [[Информация Шеннона|surprisal]]?), полученного при аппроксимации p моделью q.
$$D_{KL}(p||q) = \sum_{x\in X}\bigg[p(x)\log\frac{p(x)}{q(x)}\bigg] = H(p,q) - H(p) \leftrightarrow H(p,q) = H(p) + D_{KL}(p||q)$$
Случай $H(p) = H(p,q)$ описывает нулевой $D_{KL}$.