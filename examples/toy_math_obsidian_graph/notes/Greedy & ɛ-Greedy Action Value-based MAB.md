---
tags:
  - RL/Bandits
  - SYS/DEPRECATED
---
$$\boxed{A_t = argmax_{a\in\mathcal{A}}Q_t(a)}$$
Методы решения [[Multi-Armed Bandit|задачи]] могут быть построены на основе приближение истинного [[Action Value|action value]] $q(a)$ на каждом шаге аппроксимацией вида $Q_t(a) = \frac{R_1 + R_2 + ... + R_{N_t(a)}}{N_t(a)}$ , где $N_t(a)$ - это кол-во раз, когда было выбрано действие $a$ к моменту времени t, то есть оценка скользящего среднего для наград от каждого действия. 
#### Greedy
Жадной стратегией будет каждый раз выбирать $A_t^\star = argmax_{a\in\mathcal{A}}Q_t(a)$; жадная стратегия всегда будет максимизировать текущую прибыль, то есть **эксплуатировать**, *модель применяет всю известную в моменте информацию о распределениях чтобы максимизировать моментальную награду*. 
#### ɛ-Greedy
При принятии решения, чтобы добавить возможность **исследования** вводится параметр $\epsilon$ отвечающий за вероятность не выбрать жадно аргмакс - то есть с вероятностью $\epsilon$ модель совершит действие  $A_t^\epsilon \in \mathcal{A}\setminus \{A_t^\star\}$, выбрав другое с равномерной вероятностью.
$$A_t^\epsilon \sim f(a) = \begin{cases} 
1-\epsilon, a = argmax_{a\in\mathcal{A}}Q_t(a) \\
\epsilon\frac{1}{|\mathcal{A}|}, otherwise
\end{cases}$$
#### Сходимость
В силу закона больших чисел оценка $Q_t$ стремится к истинному значению, сходимость жадного алгоритма для оптимального действия теоретически быстрее, но он может не найти его в силу исключительной эксплуатации. Эпсилон жадные алгоритмы больше подходят для шумных наград с большими дисперсиями, но логично проигрывают обычном жадным, когда дисперсия мала. 
#### Incremental Algorithm
Построение оценки  $Q_t$ возможно инкрементальным образом, при этом инкремент может не быть 
##### Стационарный случай
$$Q_{k+1}=\frac{1}{k}\sum_{i=1}^kR_i = \frac{1}{k}\bigg(R_k+\sum_{i=1}^{k-1}R_i\bigg) = \frac{1}{k}\bigg( R_k + (k+1)Q_k+Q_k-Q_k \bigg) = \frac{1}{k}\bigg( R_k + kQ_k-Q_k\bigg)$$
$$
Q_{k+1}=\frac{1}{k}\bigg( R_k + kQ_k-Q_k\bigg) = Q_k + \frac{1}{k}\bigg[R_k-Q_k\bigg] 
$$
Таким образом 
$$Q_{k+1}=Q_k + \frac{1}{k}\bigg[R_k-Q_k\bigg]\leftrightarrow \text{new est.} = \text{old est.} + \text{stepsize}\times\big[\text{target}-\text{old. est}\big]
$$
