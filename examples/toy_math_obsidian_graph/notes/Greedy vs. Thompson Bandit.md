---
tags:
  - RL/Bandits
  - SYS/DEPRECATED
---
Бандиты, обученные с помощью [[Greedy & ɛ-Greedy Action Value-based MAB|жадности или рандомизированной жадности]] отличаются от [[Thompson Sampling|Томпсоновских (также Байессовских)]] на уровне алгоритма в действительности в 1 строчке:
___
Для жадных модель выбирается как оценка на уровне параметризации, т.е.:
$$\hat{\theta} \leftarrow \mathbb{E}_{p}[\theta]$$
Для Томпсоновских это единичная выборка из распределения:
$$\hat{\theta} \sim p: dom(p) = \Theta$$
___
При этом в жадных бандитах на основе action-value и в [[Softmax Bandit]] явным образом распределение параметров не задано, хотя последний имеет распределение Больцмана поверх action-value и фактически сэмплирует из него действие, параметр самого распределения $\tau$ зафиксирован в базовом варианте.
