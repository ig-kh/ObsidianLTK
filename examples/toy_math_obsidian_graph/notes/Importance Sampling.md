---
tags:
  - STATS
  - SYS/DEPRECATED
---
$$\boxed{\mathbb{E}_{x\sim p(x)}[G(x)]=\mathbb{E}_{x\sim q(x)}\bigg[G(x)\frac{p(x)}{q(x)}\bigg]}$$
Importance Sampling - оценка матожидания статистики по статистике, моделирующей истинное распределение (напр. когда дорого сэмплировать истинное - Казино). Выражается формулой в интегральном виде:

$$\mathbb{E}_{x\sim p(x)}[G(x)] = \int_{x\in\mathcal{X}}p(x)G(x)dx =\big[ \forall x: p(x) >0: q(x) > 0\big]= \int_{x\in\mathcal{X}}p(x)\frac{q(x)}{q(x)}G(x)dx$$
Что равно матожиданию по модельному распределению:
$$\int_{x\in\mathcal{X}}p(x)\frac{q(x)}{q(x)}G(x)dx = \int_{x\in\mathcal{X}}q(x)\frac{p(x)}{q(x)}G(x)dx = \mathbb{E}_{x\sim q(x)}\bigg[G(x)\frac{p(x)}{q(x)}\bigg]= \mathbb{E}_{q(x)}\big[G(x)w(x)\big]$$

$w(x)$ может потребовать нормировки $\hat{w}_i(x) = \frac{w_i(x)}{\sum_{x\in \mathcal{X}}w(x)}$ и называется sample importance, они отражают (перевзвешивают сэмплы в апрокимации) меру переоцененности распределением $p(x)$ некоторых сэмплов, где $w(x)>1$ и наоборот недооцененность с $w(x)<1$, и корректируют их.

Также логичным образом $q(x)$ тем лучше приближает $p(x)$, чем меньше [[Kullback-Leibler Divergence|расстояние]] $D_{KL}(p||q)$.

Дисперсия весов описывает насколько хорошо подобрано $q(x)$ в качестве аппроксимации (тяжесть хвостов, эксцентрика, симметричность).

При [[Cross Entropy (Evolutionary) Method|оптимизации распределения]] в кроссэнтропии значения весов могут полагаться единицами (в рамках грубого предположения о равномерности p и q), что стабилизирует  вычислительные процессы.