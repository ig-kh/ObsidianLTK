---
tags:
  - RL
  - SYS/DEPRECATED
---
В неформальном плане RL изучает решение класса задач, подразумевающего существование некоторой среды, с которой агент на основе алгоритма может взаимодействовать и воспринимать результат своих действий со средой.

### Базовые определения
#### Восприятие и взаимодействие
Формальнее - агент наблюдая *o* совершает действие *а* и получает награду *r*. Упорядоченная совокупность всех наблюдений и действий к моменту времени *t* есть история $H_t = o_0,a_0, ... o_t,a_t$, состояние $s_t = f(H_t)$ есть некая функциональная зависимая от истории переменная состояния среды. 
#### Состояния
$s_t^e$ - **e**nvironment **s**tate at the moment **t** - вся информация о всей среде в определенный момент (аналог - состояния всех квантов), не обязательно релевантная и воспринимаемая агентом. $s_t^a \subseteq s_t^e || s_t^a = g(s_t^e)$ - **a**gent **s**tate at the moment **t** - подмножество или функция от состояния среды, известная агенту, на ее основе он действует.

$s^e_t$ строго обладает [[Марковское свойство|Марковским свойством]] по определению, для $s_t^a$ оно не гарантировано в общем случае. 

#### Reward & Return
Награда (ревард) $r_t\in \mathbb{R}$ - численная оценка (сигнал) действия агента в некоторый момент $t$ (зависящий от состояния и действия), основной метод манипуляции моделью; $G_t = \sum_{i=0}^{\infty} r_{t+i}$ - return или кумулятивная награда (может быть дисконтирована как $G_t = \sum^{\infty}_{i=t, j=0}r_i*\gamma^j$); $t$ может восприниматься как аналогия точки на траектории истории агента-среды, т.е. кортежа состояние-действие. 
#### Policy
$\pi$ - **p**olicy - политика, правило, на основе которого агент в зависимости от среды выбирает действие, т.е. функциональная зависимость $a_{t+1} = \pi(s_t)$, может быть не детерминировано и описывать распределения как $\pi(a|s) = \mathbb{P}[\mathcal{A}=a|\mathcal{S}=s]$  и быть параметризовано  $a_{t+1} = \pi(s_t, \theta) = \pi_\theta(s_t) \cup a_{t+1} \sim \pi_\theta(a|s_t)$

#### Value
$v$ - **v**alue function - ожидание возгнаграждения от выбранной политики действия на некотором горизонте:
$$v_\pi(s) = \mathbb{E}_\pi\big[ f_{m>0}(r_0, ... r_{t+m})| \mathcal{S}_t=s \big] = \mathbb{E}_\pi\big[G_t| \mathcal{S}_t=s \big] $$
##### Пример с затуханием: $$v_\pi(s) = \mathbb{E}_\pi\bigg[\sum^{\infty}_{i=t, j=0}r_i*\gamma^j\bigg| \mathcal{S}_t=s\bigg], \gamma \in (0,1)$$
### Моделирование процесса
#### Reward-Model
$$\mathcal{R}^a_s = \mathbb{E}\big[r|\mathcal{S}=s,\mathcal{A}=a\big]$$
Ожидание вознаграждения за выполнения того или иного действия в определенном состоянии среды.
#### Transition-Model
$$\mathcal{P}^a_{s, s'} =\mathbb{P}\big[\mathcal{S}'=s|\mathcal{S}=s,\mathcal{A}=a\big]$$
Моделирование реакции среды, того, в какое состояние она придет из текущего при определенном действии, не отражает саму реальность поведения среды, но то, как агент ее воспринимает.